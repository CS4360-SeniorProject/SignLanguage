{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.10.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (24.3.6)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (49.2.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dgaytan\\appdata\\roaming\\python\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dgaytan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python numpy mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DGaytan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation # Segmentation masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential TODOs: \n",
    "# -> add hand specific segmentation for better detections\n",
    "# -> apply joint bilateral filter to results.segmentation_mask w/ image\n",
    "\n",
    "def mediapipe_segmentation(image):\n",
    "    bg_image = None                                             # Can set color or image as bg if desired\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                               # Image is no longer writeable\n",
    "    results = selfie_segmentation.process(image)                # Apply segmentation mask\n",
    "    image.flags.writeable = True                                # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)              # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    # referenced nicolai nielsen segmentation tutorial #\n",
    "    # Draw segmentation on background of video\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1 #was 0.15\n",
    "    \n",
    "    # Filter background\n",
    "    # Can apply an image or flat color instead of blur, but would need implimentation atm\n",
    "    if bg_image is None:\n",
    "        bg_image = cv2.GaussianBlur(image, (55,55),0)\n",
    "\n",
    "    output_image = np.where(condition, image, bg_image)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    " #   mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    #Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "# \\ for newline wrap on with statement\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "\n",
    "    '''\n",
    "    Test and see on gestures that have more movement if we should add the parameter model_complexity =2\n",
    "    0 it will be faster, but less accurate and if it is 2 it will be more accurate, but also slower.\n",
    "\n",
    "    '''\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(results.left_hand_landmarks.landmark)\n",
    "\n",
    "# Use pose maybe with more complicated gestures for more data \n",
    "\n",
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "#face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, pose, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Test Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "alphabets = np.array(['A', 'B', 'C'])\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'more', 'iloveyou', 'neutral'])  # Add 'neutral' to the list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no_sequences = 30  # Videos\n",
    "sequence_length = 30  # Frames as .npy files\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'more', 'iloveyou', 'neutral'])  # Include 'neutral'\n",
    "\n",
    "# Update folders_and_items to include the new action\n",
    "folders_and_items = [('action', actions)]\n",
    "\n",
    "def create_folders(folder_name, items):\n",
    "    folder_path = os.path.join(DATA_PATH, folder_name)\n",
    "    # Ensure the base folder path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created directory: {folder_path}\")\n",
    "\n",
    "    for item in items:\n",
    "        item_folder_path = os.path.join(folder_path, item)\n",
    "        # Check and create item-specific folder if it doesn't exist\n",
    "        if not os.path.exists(item_folder_path):\n",
    "            os.makedirs(item_folder_path)\n",
    "            print(f\"Created directory: {item_folder_path}\")\n",
    "\n",
    "        # Check and create sequence folders within the item folder\n",
    "        for sequence in range(1, no_sequences + 1):\n",
    "            sequence_path = os.path.join(item_folder_path, str(sequence))\n",
    "            if not os.path.exists(sequence_path):\n",
    "                os.makedirs(sequence_path)\n",
    "                print(f\"Created directory: {sequence_path}\")\n",
    "\n",
    "# Create folders for actions including the new 'neutral' category\n",
    "for folder_name, items in folders_and_items:\n",
    "    create_folders(folder_name, items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Set up video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "export_complete = False  # Flag to track completion status\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "\n",
    "    paused = False  # Initialize the pause state\n",
    "    while True:  # Infinite loop for continuous processing    \n",
    "        if not paused and not export_complete: \n",
    "            # Loop through the list of tuples\n",
    "            for folder_name, items in folders_and_items:\n",
    "                # Loop through items (actions or alphabets)\n",
    "                for item in items:\n",
    "                    # Show message for the next action or alphabet\n",
    "                    cv2.putText(image, f'Next {folder_name}: {item}', (120, 200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(3000)  # Wait for 3 seconds\n",
    "\n",
    "                    # Loop through sequences aka videos\n",
    "                    for sequence in range(1, no_sequences + 1):\n",
    "                        # Loop through video length aka sequence length\n",
    "                        for frame_num in range(sequence_length):\n",
    "                            # Read feed\n",
    "                            ret, frame = cap.read()\n",
    "\n",
    "                            # Make detections\n",
    "                            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                            # Draw landmarks\n",
    "                            draw_styled_landmarks(image, results)\n",
    "\n",
    "                            # Apply wait logic\n",
    "                            if frame_num == 0: \n",
    "                                cv2.putText(image, 'STARTING COLLECTION', (120, 200), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                                cv2.putText(image, f'Collecting frames for {item} Video Number {sequence}', (15, 12), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                                # Show to screen\n",
    "                                cv2.imshow('OpenCV Feed', image)\n",
    "                                cv2.waitKey(4000)\n",
    "                            else: \n",
    "                                cv2.putText(image, f'Collecting frames for {item} Video Number {sequence}', (15, 12), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                                # Show to screen\n",
    "                                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                            # Export keypoints\n",
    "                            if sequence in range(1, 41):  # Ensure saving keypoints only for frames within the sequence length\n",
    "                                keypoints = extract_keypoints(results)\n",
    "                                npy_path = os.path.join(DATA_PATH, folder_name, item, str(sequence), str(frame_num + 1)) + \".npy\"\n",
    "                                np.save(npy_path, keypoints)\n",
    "\n",
    "                            # Check if keypoints export is complete\n",
    "                            if sequence == no_sequences and frame_num == sequence_length - 1:\n",
    "                                export_complete = True\n",
    "\n",
    "                            # Break gracefully\n",
    "                            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                                raise KeyboardInterrupt\n",
    "\n",
    "                            # Check for pause key (spacebar)\n",
    "                            key = cv2.waitKey(1)\n",
    "                            if key == ord(' '):  # Spacebar pressed\n",
    "                                paused = not paused  # Toggle the paused state\n",
    "\n",
    "                                # If paused, wait for a key press to resume\n",
    "                                if paused:\n",
    "                                    cv2.waitKey(-1)  # Wait indefinitely for a key press\n",
    "\n",
    "        # If export is complete, break the loop\n",
    "        if export_complete:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Model with more gestures\n",
    "\n",
    "1. This only adds to already created models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Set up video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "export_complete = False  # Flag to track completion status\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "    paused = False\n",
    "    while not export_complete:\n",
    "        for folder_name, items in folders_and_items:\n",
    "            for item in items:\n",
    "                if item == 'neutral':  # Check if the current item is 'neutral'\n",
    "                    for sequence in range(1, no_sequences + 1):\n",
    "                        for frame_num in range(sequence_length):\n",
    "                            if not paused:\n",
    "                                ret, frame = cap.read()\n",
    "                                if not ret:\n",
    "                                    continue\n",
    "\n",
    "                                # Processing frame\n",
    "                                image, results = mediapipe_detection(frame, holistic)\n",
    "                                draw_styled_landmarks(image, results)\n",
    "\n",
    "                                # Display status for collecting frames\n",
    "                                cv2.putText(image, f'Collecting frames for {item}, Sequence {sequence}', \n",
    "                                            (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                                # Save keypoints\n",
    "                                keypoints = extract_keypoints(results)\n",
    "                                npy_path = os.path.join(DATA_PATH, folder_name, item, str(sequence), f\"{frame_num + 1}.npy\")\n",
    "                                np.save(npy_path, keypoints)\n",
    "\n",
    "                                # Check for exit or pause conditions\n",
    "                                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                                    export_complete = True\n",
    "                                    break\n",
    "                                if cv2.waitKey(1) == ord(' '):  # Pause on spacebar\n",
    "                                    cv2.waitKey(-1)  # Wait indefinitely until another key is pressed\n",
    "\n",
    "                            if export_complete:\n",
    "                                break\n",
    "                        if export_complete:\n",
    "                            break\n",
    "                if export_complete:\n",
    "                    break\n",
    "            if export_complete:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_map = {label:num for num, label in enumerate(alphabets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "alphabet_folder_path = os.path.join(DATA_PATH, 'alphabet') \n",
    "\n",
    "\n",
    "# Initialize the sequences and labels lists\n",
    "act_sequences, act_list_labels = [], []\n",
    "\n",
    "# Loop through the alphabets\n",
    "for alphabet in alphabets:\n",
    "    # Get the sequence directories for the current alphabet\n",
    "    sequence_dirs = os.listdir(os.path.join(alphabet_folder_path, alphabet))\n",
    "    \n",
    "    # Loop through the sequence directories\n",
    "    for sequence_dir in sequence_dirs:\n",
    "        # Initialize the window list for the current sequence\n",
    "        window = []\n",
    "        \n",
    "        # Loop through the .npy files in the current sequence directory\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Load the .npy file\n",
    "            res = np.load(os.path.join(DATA_PATH, 'alphabet', alphabet, sequence_dir, \"{}.npy\".format(frame_num + 1)))\n",
    "            \n",
    "            # Append the loaded .npy file to the window list\n",
    "            window.append(res)\n",
    "        \n",
    "        # Append the window list to the sequences list\n",
    "        act_sequences.append(window)\n",
    "        \n",
    "        # Append the label for the current alphabet to the labels list\n",
    "        act_list_labels.append(alphabet_map[alphabet]) # Use alphabet as the label\n",
    "\n",
    "\n",
    "        \"Number of sequences and labels should be the number of items inside the gesture folder\"\n",
    "\n",
    "print(\"Number of sequences:\", len(act_sequences))\n",
    "print(\"Number of labels:\", len(act_list_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if each sequence has the same amount of sequence and keypoints\n",
    "for i, seq in enumerate(act_sequences):\n",
    "    print(f\"Sequence {i} shape: {np.array(seq).shape}\")\n",
    "    if np.array(seq).shape != (sequence_length, 258):  # keypoints\n",
    "        print(f\"Error in sequence {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(act_sequences):\n",
    "    seq_array = np.array(seq)\n",
    "    print(f\"Sequence {i} shape: {seq_array.shape}, dtype: {seq_array.dtype}\")\n",
    "    if seq_array.shape != (sequence_length, 258):  # Assuming each frame represented by 258 keypoints\n",
    "        print(f\"Error in sequence {i} with shape {seq_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(act_list_labels).astype(int)\n",
    "\n",
    "# Ensure shapes are consistent\n",
    "print(\"Shapes of sequences and labels:\")\n",
    "print(\"Sequences shape:\", np.array(act_sequences).shape)\n",
    "print(\"Labels shape:\", np.array(act_list_labels).shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = np.array(act_sequences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Verify shapes after splitting\n",
    "print(\"Shapes after splitting:\")\n",
    "print(\"X_train shape:\", X_train.shape)  # Labels of train shape\n",
    "print(\"X_test shape:\", X_test.shape)   # Labels of 5 random test \n",
    "print(\"y_train shape:\", y_train.shape)      #(train shape, classes)\n",
    "print(\"y_test shape:\", y_test.shape)       \n",
    "\n",
    "# Inspect testing labels\n",
    "print(\"Testing labels:\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the testing set and print labels and sequences\n",
    "for i in range(len(y_test)):\n",
    "    label = y_test[i]\n",
    "    sequence = X_test[i]\n",
    "    print(f\"Label: {label}, Sequence Frame: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Model names with gestures\n",
    "\n",
    "#alphabets = np.array(['A', 'B', 'C'])\n",
    "actions = np.array(['hello', 'more', 'iloveyou', 'neutral'])  # Add 'neutral' to the list\n",
    "\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Model Labels'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Save the variables to a file using pickle\n",
    "data = {\n",
    "    'actions': actions,\n",
    "    #'alphabets': alphabets,\n",
    "}\n",
    "\n",
    "file_path = os.path.join(folder_path, 'model_data.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "\n",
    "Epoch at 20000 is a little high for low data. STOP training early if accuracy is acceptable and loss has stopped cosistently decreashing\n",
    "\n",
    "tb_callback: should all fit in our memory for now\n",
    "\n",
    "while the model is running and being trained. You can look at the tensorboard call back\n",
    "* Check the Logs folder/events.out\n",
    "* go to cmd\n",
    "* cd into the log/train\n",
    "* type: tensorboard --logdir=.\n",
    "* it will give you a localhost url that will give you graph of the model:  http://localhost:6006/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for alphabets: input_shape=(30,126)))\n",
    "action_model = Sequential()\n",
    "action_model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258)))\n",
    "action_model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "action_model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "action_model.add(Dense(64, activation='relu'))\n",
    "action_model.add(Dense(32, activation='relu'))\n",
    "action_model.add(Dense(actions.shape[0], activation='softmax'))  # actions.shape number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_model.fit(X_train, y_train, epochs=500, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "# Get predictions for the test set\n",
    "res = action_model.predict(X_test)\n",
    "\n",
    "# Print the predicted action for the 5th sample in the test set\n",
    "print(\"Predicted action:\", actions[np.argmax(res[4])])\n",
    "\n",
    "# Print the actual action for the 5th sample in the test set\n",
    "print(\"Actual action:\", actions[np.argmax(y_test[4])])\n",
    "\n",
    "# Convert predictions and true labels to indices\n",
    "yhat = np.argmax(res, axis=1)\n",
    "ytrue = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the multilabel confusion matrix\n",
    "confusion_matrix = multilabel_confusion_matrix(ytrue, yhat)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(ytrue, yhat)\n",
    "print(\"Accuracy Score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "models_folder = 'models'\n",
    "\n",
    "action_model.save(os.path.join(models_folder,'action.h5'))  # change this per model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Load Models and Labels to Run in Real Time if they are not stored in the Kernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the models in Real Time\n",
    "\n",
    "1. Run all the cell blocks in section 1-3 \n",
    "  - This will load all the libraries and methods to extract landmarks\n",
    "\n",
    "\n",
    "2. Load the labels and the models in this section 10. \n",
    "\n",
    "3 Now you should be able to test the cell blocks in \n",
    "  -Test One Model \n",
    "  -Test Two Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for each models\n",
    "\n",
    "alphabets = (['A', 'B', 'C'])\n",
    "actions = np.array(['hello', 'more', 'iloveyou', 'neutral'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DGaytan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Action model loaded successfully.\n",
      "Alphabet model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "def load_model_safely(model_path):\n",
    "    try:\n",
    "        return load_model(model_path), None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "models_folder = 'models'\n",
    "action_model, err = load_model_safely(os.path.join(models_folder, 'action.h5'))\n",
    "if action_model:\n",
    "    print(\"Action model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error loading action model: {err}\")\n",
    "\n",
    "alphabet_model, err = load_model_safely(os.path.join(models_folder, 'alphabet.h5'))\n",
    "if alphabet_model:\n",
    "    print(\"Alphabet model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error loading alphabet model: {err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time\n",
    "\n",
    "1. Only one model\n",
    "2. Two modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            # Testing One Model Only\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "#with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "              # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label = alphabets[np.argmax(res)]\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "            if np.unique(predictions[-5:])[0] == predicted_label and res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label != sentence[-1]:\n",
    "                        sentence.append(predicted_label)\n",
    "                else:\n",
    "                    sentence.append(predicted_label)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "            # Display the label text only\n",
    "            \n",
    "            image = prob_viz(predicted_label, image)\n",
    "\n",
    "\n",
    "            # Show the image with the label text\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                  #Test two models\n",
    "# Define the prob_viz function to handle both alphabet and action labels\n",
    "def prob_viz(prediction_label, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    cv2.putText(output_frame, prediction_label, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "# Testing Both Models\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 1\n",
    "current_prediction = \"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Initialize predictions list for each iteration\n",
    "        predictions = []\n",
    "\n",
    "        # Prediction logic for the alphabet model\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            # Prediction logic for the alphabet model\n",
    "            res_alphabet = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label_alphabet = alphabets[np.argmax(res_alphabet)]\n",
    "            predictions.append(predicted_label_alphabet)\n",
    "            \n",
    "            # Logic for sentence formation\n",
    "            if np.unique(predictions[-5:])[0] == predicted_label_alphabet and res_alphabet[np.argmax(res_alphabet)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label_alphabet != sentence[-1]:\n",
    "                        sentence.append(predicted_label_alphabet)\n",
    "                else:\n",
    "                    sentence.append(predicted_label_alphabet)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Predict using the action model\n",
    "            res_action = action_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label_action = actions[np.argmax(res_action)]\n",
    "            predictions.append(predicted_label_action)\n",
    "\n",
    "            # Display the label text for both alphabet and action\n",
    "            current_prediction = \" \".join(predictions)\n",
    "            image = prob_viz(current_prediction, image)\n",
    "\n",
    "        # Show the image with the label text\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.  Improved Testing with \"neutral\" model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Constants\n",
    "NEUTRAL_ACTION_INDEX = 4  # Update this based on your model's 'neutral' index\n",
    "CONFIDENCE_THRESHOLD = 0.97  # Confidence threshold to accept predictions\n",
    "\n",
    "# Initialize video capture and MediaPipe model\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    sequence_action = []\n",
    "    sequence_alphabet = []\n",
    "    label_action = ''\n",
    "    label_alphabet = ''\n",
    "    display_text = ''\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        keypoints_action = extract_keypoints(results, \"action\")\n",
    "        keypoints_alphabet = extract_keypoints(results, \"alphabet\")\n",
    "        sequence_action.append(keypoints_action)\n",
    "        sequence_alphabet.append(keypoints_alphabet)\n",
    "\n",
    "        if len(sequence_action) == 30:\n",
    "            pred_action = action_model.predict(np.expand_dims(sequence_action, axis=0), verbose=0)[0]\n",
    "            action_confidence = np.max(pred_action)\n",
    "            action_index = np.argmax(pred_action)\n",
    "            sequence_action.pop(0)  # Remove the oldest frame\n",
    "\n",
    "            if action_index != NEUTRAL_ACTION_INDEX and action_confidence > CONFIDENCE_THRESHOLD:\n",
    "                label_action = actions[action_index]\n",
    "                # Now check the alphabet model\n",
    "                if len(sequence_alphabet) == 30:\n",
    "                    pred_alphabet = alphabet_model.predict(np.expand_dims(sequence_alphabet, axis=0), verbose=0)[0]\n",
    "                    alphabet_confidence = np.max(pred_alphabet)\n",
    "                    if alphabet_confidence > CONFIDENCE_THRESHOLD:\n",
    "                        label_alphabet = alphabets[np.argmax(pred_alphabet)]\n",
    "                        display_text = f\"Action: {label_action}, Alphabet: {label_alphabet}\"\n",
    "                    sequence_alphabet.pop(0)  # Remove the oldest frame\n",
    "            else:\n",
    "                label_action = 'Neutral'  # Or just use an empty string ''\n",
    "                display_text = ''\n",
    "\n",
    "        image = prob_viz([display_text], image)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            # Web\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "\n",
    "models_folder = 'models'\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation # Segmentation masking\n",
    "\n",
    "# potential TODOs: \n",
    "# -> add hand specific segmentation for better detections\n",
    "# -> apply joint bilateral filter to results.segmentation_mask w/ image\n",
    "\n",
    "def mediapipe_segmentation(image):\n",
    "    bg_image = None                                             # Can set color or image as bg if desired\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                               # Image is no longer writeable\n",
    "    results = selfie_segmentation.process(image)                # Apply segmentation mask\n",
    "    image.flags.writeable = True                                # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)              # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    # referenced nicolai nielsen segmentation tutorial #\n",
    "    # Draw segmentation on background of video\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1 #was 0.15\n",
    "    \n",
    "    # Filter background\n",
    "    # Can apply an image or flat color instead of blur, but would need implimentation atm\n",
    "    if bg_image is None:\n",
    "        bg_image = cv2.GaussianBlur(image, (55,55),0)\n",
    "\n",
    "    output_image = np.where(condition, image, bg_image)\n",
    "    return output_image\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "def extract_keypoints(results):\n",
    "   # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "   # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "\n",
    "\n",
    "# Load the alphabet model\n",
    "alphabet_model_filename = 'alphabet.h5'\n",
    "alphabet_model_filepath = os.path.join(models_folder, alphabet_model_filename)\n",
    "try:\n",
    "    alphabet_model = load_model(alphabet_model_filepath)\n",
    "    print(f\"Alphabet model {alphabet_model_filename} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading alphabet model {alphabet_model_filename}: {str(e)}\")\n",
    "    \n",
    "alphabets = (['A', 'B', 'C'])\n",
    "\n",
    "                                            # Testing One Model Only\n",
    "                                            # Testing One Model Only\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "#with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "              # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        #draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label = alphabets[np.argmax(res)]\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "            if np.unique(predictions[-5:])[0] == predicted_label and res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label != sentence[-1]:\n",
    "                        sentence.append(predicted_label)\n",
    "                else:\n",
    "                    sentence.append(predicted_label)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "            # Display the label text only\n",
    "            \n",
    "            image = prob_viz(predicted_label, image)\n",
    "\n",
    "\n",
    "            # Show the image with the label text\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

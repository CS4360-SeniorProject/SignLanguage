{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow opencv-python numpy mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation # Segmentation masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential TODOs: \n",
    "# -> add hand specific segmentation for better detections\n",
    "# -> apply joint bilateral filter to results.segmentation_mask w/ image\n",
    "\n",
    "def mediapipe_segmentation(image):\n",
    "    bg_image = None                                             # Can set color or image as bg if desired\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                               # Image is no longer writeable\n",
    "    results = selfie_segmentation.process(image)                # Apply segmentation mask\n",
    "    image.flags.writeable = True                                # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)              # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    # referenced nicolai nielsen segmentation tutorial #\n",
    "    # Draw segmentation on background of video\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1 #was 0.15\n",
    "    \n",
    "    # Filter background\n",
    "    # Can apply an image or flat color instead of blur, but would need implimentation atm\n",
    "    if bg_image is None:\n",
    "        bg_image = cv2.GaussianBlur(image, (55,55),0)\n",
    "\n",
    "    output_image = np.where(condition, image, bg_image)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    " #   mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    " #   mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # # Draw pose connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "# \\ for newline wrap on with statement\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "\n",
    "    '''\n",
    "    Test and see on gestures that have more movement if we should add the parameter model_complexity =2\n",
    "    0 it will be faster, but less accurate and if it is 2 it will be more accurate, but also slower.\n",
    "\n",
    "    '''\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "import cv2\n",
    "\n",
    "# IP address and port number of your Android device running IP Webcam\n",
    "url = 'https://10.0.0.144:8080/video'\n",
    "\n",
    "# Create a VideoCapture object with the URL of the IP Webcam stream\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "# Check if the camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open the camera.\")\n",
    "else:\n",
    "    print(\"Camera opened successfully.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from camera.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Android Camera Feed', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(results.left_hand_landmarks.landmark)\n",
    "\n",
    "# Use pose maybe with more complicated gestures for more data \n",
    "'''\n",
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "#face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "   # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "   # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Test Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "alphabets = np.array(['A', 'B', 'C'])\n",
    "actions = np.array(['hello', 'more', 'iloveyou'])\n",
    "\n",
    "\n",
    "\n",
    "no_sequences = 30  # Videos\n",
    "sequence_length = 30  # Frames as .npy files\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples containing folder names and corresponding items\n",
    "folders_and_items = [('alphabet', alphabets)]\n",
    "\n",
    "def create_folders(folder_name, items):\n",
    "    folder_path = os.path.join(DATA_PATH, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created directory: {folder_path}\")\n",
    "\n",
    "    for item in items:  \n",
    "        item_folder_path = os.path.join(folder_path, item)                                       \n",
    "        if not os.path.exists(item_folder_path):\n",
    "            os.makedirs(item_folder_path)\n",
    "            print(f\"Created directory: {item_folder_path}\")\n",
    "\n",
    "        # Create sequence folders inside each item folder\n",
    "        for sequence in range(1, no_sequences + 1):\n",
    "            sequence_path = os.path.join(item_folder_path, str(sequence))\n",
    "            if not os.path.exists(sequence_path):\n",
    "                os.makedirs(sequence_path)\n",
    "                print(f\"Created directory: {sequence_path}\")\n",
    "\n",
    "# Create folders for actions and alphabets\n",
    "for folder_name, items in folders_and_items:\n",
    "    create_folders(folder_name, items)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Set up video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "export_complete = False  # Flag to track completion status\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "\n",
    "    paused = False  # Initialize the pause state\n",
    "    while True:  # Infinite loop for continuous processing    \n",
    "        if not paused and not export_complete: \n",
    "            # Loop through the list of tuples\n",
    "            for folder_name, items in folders_and_items:\n",
    "                # Loop through items (actions or alphabets)\n",
    "                for item in items:\n",
    "                    # Show message for the next action or alphabet\n",
    "                    cv2.putText(image, f'Next {folder_name}: {item}', (120, 200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(3000)  # Wait for 3 seconds\n",
    "\n",
    "                    # Loop through sequences aka videos\n",
    "                    for sequence in range(1, no_sequences + 1):\n",
    "                        # Loop through video length aka sequence length\n",
    "                        for frame_num in range(sequence_length):\n",
    "                            # Read feed\n",
    "                            ret, frame = cap.read()\n",
    "\n",
    "                            # Make detections\n",
    "                            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                            # Draw landmarks\n",
    "                            draw_styled_landmarks(image, results)\n",
    "\n",
    "                            # Apply wait logic\n",
    "                            if frame_num == 0: \n",
    "                                cv2.putText(image, 'STARTING COLLECTION', (120, 200), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                                cv2.putText(image, f'Collecting frames for {item} Video Number {sequence}', (15, 12), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                                # Show to screen\n",
    "                                cv2.imshow('OpenCV Feed', image)\n",
    "                                cv2.waitKey(4000)\n",
    "                            else: \n",
    "                                cv2.putText(image, f'Collecting frames for {item} Video Number {sequence}', (15, 12), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                                # Show to screen\n",
    "                                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                            # Export keypoints\n",
    "                            if sequence in range(1, 41):  # Ensure saving keypoints only for frames within the sequence length\n",
    "                                keypoints = extract_keypoints(results)\n",
    "                                npy_path = os.path.join(DATA_PATH, folder_name, item, str(sequence), str(frame_num + 1)) + \".npy\"\n",
    "                                np.save(npy_path, keypoints)\n",
    "\n",
    "                            # Check if keypoints export is complete\n",
    "                            if sequence == no_sequences and frame_num == sequence_length - 1:\n",
    "                                export_complete = True\n",
    "\n",
    "                            # Break gracefully\n",
    "                            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                                raise KeyboardInterrupt\n",
    "\n",
    "                            # Check for pause key (spacebar)\n",
    "                            key = cv2.waitKey(1)\n",
    "                            if key == ord(' '):  # Spacebar pressed\n",
    "                                paused = not paused  # Toggle the paused state\n",
    "\n",
    "                                # If paused, wait for a key press to resume\n",
    "                                if paused:\n",
    "                                    cv2.waitKey(-1)  # Wait indefinitely for a key press\n",
    "\n",
    "        # If export is complete, break the loop\n",
    "        if export_complete:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_map = {label:num for num, label in enumerate(alphabets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "alphabet_folder_path = os.path.join(DATA_PATH, 'alphabet') \n",
    "\n",
    "\n",
    "# Initialize the sequences and labels lists\n",
    "alph_sequences, alph_list_labels = [], []\n",
    "\n",
    "# Loop through the alphabets\n",
    "for alphabet in alphabets:\n",
    "    # Get the sequence directories for the current alphabet\n",
    "    sequence_dirs = os.listdir(os.path.join(alphabet_folder_path, alphabet))\n",
    "    \n",
    "    # Loop through the sequence directories\n",
    "    for sequence_dir in sequence_dirs:\n",
    "        # Initialize the window list for the current sequence\n",
    "        window = []\n",
    "        \n",
    "        # Loop through the .npy files in the current sequence directory\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Load the .npy file\n",
    "            res = np.load(os.path.join(DATA_PATH, 'alphabet', alphabet, sequence_dir, \"{}.npy\".format(frame_num + 1)))\n",
    "            \n",
    "            # Append the loaded .npy file to the window list\n",
    "            window.append(res)\n",
    "        \n",
    "        # Append the window list to the sequences list\n",
    "        alph_sequences.append(window)\n",
    "        \n",
    "        # Append the label for the current alphabet to the labels list\n",
    "        alph_list_labels.append(alphabet_map[alphabet]) # Use alphabet as the label\n",
    "\n",
    "\n",
    "        \"Number of sequences and labels should be the number of items inside the gesture folder\"\n",
    "\n",
    "print(\"Number of sequences:\", len(alph_sequences))\n",
    "print(\"Number of labels:\", len(alph_list_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(alph_list_labels).astype(int)\n",
    "\n",
    "# Ensure shapes are consistent\n",
    "print(\"Shapes of sequences and labels:\")\n",
    "print(\"Sequences shape:\", np.array(alph_sequences).shape)\n",
    "print(\"Labels shape:\", np.array(alph_list_labels).shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = np.array(alph_sequences)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Verify shapes after splitting\n",
    "print(\"Shapes after splitting:\")\n",
    "print(\"X_train shape:\", X_train.shape)  # Labels of train shape\n",
    "print(\"X_test shape:\", X_test.shape)   # Labels of 5 random test \n",
    "print(\"y_train shape:\", y_train.shape)      #(train shape, classes)\n",
    "print(\"y_test shape:\", y_test.shape)       \n",
    "\n",
    "# Inspect testing labels\n",
    "print(\"Testing labels:\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the testing set and print labels and sequences\n",
    "for i in range(len(y_test)):\n",
    "    label = y_test[i]\n",
    "    sequence = X_test[i]\n",
    "    print(f\"Label: {label}, Sequence Frame: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Model names with gestures\n",
    "# actions = np.array(['nothing', 'hello', 'more'])\n",
    "alphabets = np.array(['A', 'B', 'C'])\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Model Labels'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Save the variables to a file using pickle\n",
    "data = {\n",
    "    # 'actions': actions,\n",
    "    'alphabets': alphabets,\n",
    "}\n",
    "\n",
    "file_path = os.path.join(folder_path, 'model_data.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_model = Sequential()\n",
    "alphabet_model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,126)))\n",
    "alphabet_model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "alphabet_model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "alphabet_model.add(Dense(64, activation='relu'))\n",
    "alphabet_model.add(Dense(32, activation='relu'))\n",
    "alphabet_model.add(Dense(alphabets.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphabet_model.fit(X_train, y_train, epochs=500, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "# Get predictions for the test set\n",
    "res = alphabet_model.predict(X_test)\n",
    "\n",
    "# Print the predicted action for the 5th sample in the test set\n",
    "print(\"Predicted action:\", alphabets[np.argmax(res[4])])\n",
    "\n",
    "# Print the actual action for the 5th sample in the test set\n",
    "print(\"Actual action:\", alphabets[np.argmax(y_test[4])])\n",
    "\n",
    "# Convert predictions and true labels to indices\n",
    "yhat = np.argmax(res, axis=1)\n",
    "ytrue = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the multilabel confusion matrix\n",
    "confusion_matrix = multilabel_confusion_matrix(ytrue, yhat)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(ytrue, yhat)\n",
    "print(\"Accuracy Score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "models_folder = 'models'\n",
    "\n",
    "alphabet_model.save(os.path.join(models_folder,'alphabet.h5'))  # change this per model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Load Models and Labels to Run in Real Time if they are not stored in the Kernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the models in Real Time\n",
    "\n",
    "1. Run all the cell blocks in section 1-3 \n",
    "  - This will load all the libraries and methods to extract landmarks\n",
    "\n",
    "\n",
    "2. Load the labels and the models in this section 10. \n",
    "\n",
    "3 Now you should be able to test the cell blocks in \n",
    "  -Test One Model \n",
    "  -Test Two Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for each models\n",
    "\n",
    "alphabets = (['A', 'B', 'C'])\n",
    "actions = (['hello', 'more', 'iloveyou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Inport the previous models after restart or when we train a new model to load the previous model\"\n",
    "\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Define the folder containing the models\n",
    "models_folder = 'models'\n",
    "\n",
    "# # Load the action model\n",
    "action_model_filename = 'action.h5'\n",
    "action_model_filepath = os.path.join(models_folder, action_model_filename)\n",
    "try:\n",
    "     action_model = load_model(action_model_filepath)\n",
    "     print(f\"Action model {action_model_filename} loaded successfully.\")\n",
    "except Exception as e:\n",
    "     print(f\"Error loading action model {action_model_filename}: {str(e)}\")\n",
    "\n",
    "# Load the alphabet model\n",
    "alphabet_model_filename = 'alphabet.h5'\n",
    "alphabet_model_filepath = os.path.join(models_folder, alphabet_model_filename)\n",
    "try:\n",
    "    alphabet_model = load_model(alphabet_model_filepath)\n",
    "    print(f\"Alphabet model {alphabet_model_filename} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading alphabet model {alphabet_model_filename}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "                                            # Testing One Model Only\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "#with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "              # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label = alphabets[np.argmax(res)]\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "            if np.unique(predictions[-5:])[0] == predicted_label and res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label != sentence[-1]:\n",
    "                        sentence.append(predicted_label)\n",
    "                else:\n",
    "                    sentence.append(predicted_label)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "            # Display the label text only\n",
    "            \n",
    "            image = prob_viz(predicted_label, image)\n",
    "\n",
    "\n",
    "            # Show the image with the label text\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    #Test two models\n",
    "# Define the prob_viz function to handle both alphabet and action labels\n",
    "def prob_viz(prediction_label, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    cv2.putText(output_frame, prediction_label, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "# Testing Both Models\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 1\n",
    "current_prediction = \"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Initialize predictions list for each iteration\n",
    "        predictions = []\n",
    "\n",
    "        # Prediction logic for the alphabet model\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            # Prediction logic for the alphabet model\n",
    "            res_alphabet = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label_alphabet = alphabets[np.argmax(res_alphabet)]\n",
    "            predictions.append(predicted_label_alphabet)\n",
    "            \n",
    "            # Logic for sentence formation\n",
    "            if np.unique(predictions[-5:])[0] == predicted_label_alphabet and res_alphabet[np.argmax(res_alphabet)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label_alphabet != sentence[-1]:\n",
    "                        sentence.append(predicted_label_alphabet)\n",
    "                else:\n",
    "                    sentence.append(predicted_label_alphabet)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Predict using the action model\n",
    "            res_action = action_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label_action = actions[np.argmax(res_action)]\n",
    "            predictions.append(predicted_label_action)\n",
    "\n",
    "            # Display the label text for both alphabet and action\n",
    "            current_prediction = \" \".join(predictions)\n",
    "            image = prob_viz(current_prediction, image)\n",
    "\n",
    "        # Show the image with the label text\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DGaytan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DGaytan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Alphabet model alphabet.h5 loaded successfully.\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "                                            # Testing One Model Only\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "\n",
    "models_folder = 'models'\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation # Segmentation masking\n",
    "\n",
    "# potential TODOs: \n",
    "# -> add hand specific segmentation for better detections\n",
    "# -> apply joint bilateral filter to results.segmentation_mask w/ image\n",
    "\n",
    "def mediapipe_segmentation(image):\n",
    "    bg_image = None                                             # Can set color or image as bg if desired\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                               # Image is no longer writeable\n",
    "    results = selfie_segmentation.process(image)                # Apply segmentation mask\n",
    "    image.flags.writeable = True                                # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)              # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    # referenced nicolai nielsen segmentation tutorial #\n",
    "    # Draw segmentation on background of video\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1 #was 0.15\n",
    "    \n",
    "    # Filter background\n",
    "    # Can apply an image or flat color instead of blur, but would need implimentation atm\n",
    "    if bg_image is None:\n",
    "        bg_image = cv2.GaussianBlur(image, (55,55),0)\n",
    "\n",
    "    output_image = np.where(condition, image, bg_image)\n",
    "    return output_image\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "def extract_keypoints(results):\n",
    "   # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "   # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "\n",
    "\n",
    "# Load the alphabet model\n",
    "alphabet_model_filename = 'alphabet.h5'\n",
    "alphabet_model_filepath = os.path.join(models_folder, alphabet_model_filename)\n",
    "try:\n",
    "    alphabet_model = load_model(alphabet_model_filepath)\n",
    "    print(f\"Alphabet model {alphabet_model_filename} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading alphabet model {alphabet_model_filename}: {str(e)}\")\n",
    "    \n",
    "alphabets = (['A', 'B', 'C'])\n",
    "\n",
    "                                            # Testing One Model Only\n",
    "                                            # Testing One Model Only\n",
    "# Define the prob_viz function\n",
    "def prob_viz(prediction_labels, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    y_offset = 60\n",
    "    for i, label in enumerate(prediction_labels):\n",
    "        cv2.putText(output_frame, label, (20, y_offset + i * 40), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "#with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic, \\\n",
    "    mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as selfie_segmentation:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "              # Segment video background\n",
    "        frame = mediapipe_segmentation(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        #draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = alphabet_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_label = alphabets[np.argmax(res)]\n",
    "            predictions.append(predicted_label)\n",
    "            \n",
    "            if np.unique(predictions[-5:])[0] == predicted_label and res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if predicted_label != sentence[-1]:\n",
    "                        sentence.append(predicted_label)\n",
    "                else:\n",
    "                    sentence.append(predicted_label)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "            # Display the label text only\n",
    "            \n",
    "            image = prob_viz(predicted_label, image)\n",
    "\n",
    "\n",
    "            # Show the image with the label text\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
